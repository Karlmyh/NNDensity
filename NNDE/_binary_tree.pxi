#!python# Binary Tree Stucture# =====================##    Author: Jake Vanderplas <jakevdp@cs.washington.edu>, 2012-2013#    License: BSD##    This file is modified from scikit learn package. #    See sklearn/neighbors/_binary_tree.pxi for detail. ## =====================##    Reference:##    Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, #    Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, #    Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, #    Arnaud Joly, Brian Holt, and Gaël Varoquaux. API design #    for machine learning software: experiences from the scikit-learn #    project. In ECML PKDD Workshop: Languages for Data Mining and #    Machine Learning, pages 108–122, 2013.cimport numpy as cnpfrom libc.math cimport fabs, sqrt, exp, cos, pow, log, lgammafrom libc.math cimport fmin, fmaxfrom libc.stdlib cimport calloc, malloc, freefrom libc.string cimport memcpyimport numpy as npimport warningsctypedef cnp.float64_t DTYPE_tctypedef cnp.intp_t ITYPE_tctypedef cnp.int32_t SPARSE_INDEX_TYPE_tITYPE = np.intpDTYPE = np.float64from AKNN._partition_nodes cimport partition_node_indices####################################################################### Metric cdef inline DTYPE_t euclidean_dist(    const DTYPE_t* x1,    const DTYPE_t* x2,    ITYPE_t size,) nogil except -1:    cdef DTYPE_t tmp, d=0    cdef cnp.intp_t j    for j in range(size):        tmp = <DTYPE_t> (x1[j] - x2[j])        d += tmp * tmp    return sqrt(d)cdef inline DTYPE_t euclidean_rdist(    const DTYPE_t* x1,    const DTYPE_t* x2,    ITYPE_t size,) nogil except -1:    cdef DTYPE_t tmp, d=0    cdef cnp.intp_t j    for j in range(size):        tmp = <DTYPE_t>(x1[j] - x2[j])        d += tmp * tmp    return d######################################################################from sklearn.utils import check_array######################################################################from cython cimport floatingcdef inline void dual_swap(    floating* darr,    ITYPE_t *iarr,    ITYPE_t a,    ITYPE_t b,) nogil:    """Swap the values at index a and b of both darr and iarr"""    cdef floating dtmp = darr[a]    darr[a] = darr[b]    darr[b] = dtmp    cdef ITYPE_t itmp = iarr[a]    iarr[a] = iarr[b]    iarr[b] = itmpcdef int simultaneous_sort(    floating* values,    ITYPE_t* indices,    ITYPE_t size,) nogil:    """    Perform a recursive quicksort on the values array as to sort them ascendingly.    This simultaneously performs the swaps on both the values and the indices arrays.    The numpy equivalent is:        def simultaneous_sort(dist, idx):             i = np.argsort(dist)             return dist[i], idx[i]    Notes    -----    Arrays are manipulated via a pointer to there first element and their size    as to ease the processing of dynamically allocated buffers.    """    cdef:        ITYPE_t pivot_idx, i, store_idx        floating pivot_val    # in the small-array case, do things efficiently    if size <= 1:        pass    elif size == 2:        if values[0] > values[1]:            dual_swap(values, indices, 0, 1)    elif size == 3:        if values[0] > values[1]:            dual_swap(values, indices, 0, 1)        if values[1] > values[2]:            dual_swap(values, indices, 1, 2)            if values[0] > values[1]:                dual_swap(values, indices, 0, 1)    else:        # Determine the pivot using the median-of-three rule.        # The smallest of the three is moved to the beginning of the array,        # the middle (the pivot value) is moved to the end, and the largest        # is moved to the pivot index.        pivot_idx = size // 2        if values[0] > values[size - 1]:            dual_swap(values, indices, 0, size - 1)        if values[size - 1] > values[pivot_idx]:            dual_swap(values, indices, size - 1, pivot_idx)            if values[0] > values[size - 1]:                dual_swap(values, indices, 0, size - 1)        pivot_val = values[size - 1]        # Partition indices about pivot.  At the end of this operation,        # pivot_idx will contain the pivot value, everything to the left        # will be smaller, and everything to the right will be larger.        store_idx = 0        for i in range(size - 1):            if values[i] < pivot_val:                dual_swap(values, indices, i, store_idx)                store_idx += 1        dual_swap(values, indices, store_idx, size - 1)        pivot_idx = store_idx        # Recursively sort each side of the pivot        if pivot_idx > 1:            simultaneous_sort(values, indices, pivot_idx)        if pivot_idx + 2 < size:            simultaneous_sort(values + pivot_idx + 1,                              indices + pivot_idx + 1,                              size - pivot_idx - 1)    return 0######################################################################cdef extern from "numpy/arrayobject.h":    void PyArray_ENABLEFLAGS(cnp.ndarray arr, int flags)cnp.import_array()# some handy constantscdef DTYPE_t INF = np.infcdef DTYPE_t NEG_INF = -np.infcdef DTYPE_t PI = np.picdef DTYPE_t ROOT_2PI = sqrt(2 * PI)cdef DTYPE_t LOG_PI = log(PI)cdef DTYPE_t LOG_2PI = log(2 * PI)# Some compound datatypes used below:cdef struct NodeHeapData_t:    DTYPE_t val    ITYPE_t i1    ITYPE_t i2# build the corresponding numpy dtype for NodeHeapDatacdef NodeHeapData_t nhd_tmpNodeHeapData = np.asarray(<NodeHeapData_t[:1]>(&nhd_tmp)).dtypecdef struct NodeData_t:    ITYPE_t idx_start    ITYPE_t idx_end    ITYPE_t is_leaf    DTYPE_t radius# build the corresponding numpy dtype for NodeDatacdef NodeData_t nd_tmpNodeData = np.asarray(<NodeData_t[:1]>(&nd_tmp)).dtype####################################################################### heap push functioncdef inline int heap_push(    floating* values,    ITYPE_t* indices,    ITYPE_t size,    floating val,    ITYPE_t val_idx,) nogil:    """Push a tuple (val, val_idx) onto a fixed-size max-heap.    The max-heap is represented as a Structure of Arrays where:     - values is the array containing the data to construct the heap with     - indices is the array containing the indices (meta-data) of each value    Notes    -----    Arrays are manipulated via a pointer to there first element and their size    as to ease the processing of dynamically allocated buffers.    For instance, in pseudo-code:        values = [1.2, 0.4, 0.1],        indices = [42, 1, 5],        heap_push(            values=values,            indices=indices,            size=3,            val=0.2,            val_idx=4,        )    will modify values and indices inplace, giving at the end of the call:        values  == [0.4, 0.2, 0.1]        indices == [1, 4, 5]    """    cdef:        ITYPE_t current_idx, left_child_idx, right_child_idx, swap_idx    # Check if val should be in heap    if val >= values[0]:        return 0    # Insert val at position zero    values[0] = val    indices[0] = val_idx    # Descend the heap, swapping values until the max heap criterion is met    current_idx = 0    while True:        left_child_idx = 2 * current_idx + 1        right_child_idx = left_child_idx + 1        if left_child_idx >= size:            break        elif right_child_idx >= size:            if values[left_child_idx] > val:                swap_idx = left_child_idx            else:                break        elif values[left_child_idx] >= values[right_child_idx]:            if val < values[left_child_idx]:                swap_idx = left_child_idx            else:                break        else:            if val < values[right_child_idx]:                swap_idx = right_child_idx            else:                break        values[current_idx] = values[swap_idx]        indices[current_idx] = indices[swap_idx]        current_idx = swap_idx    values[current_idx] = val    indices[current_idx] = val_idx    return 0# TODO: add commentscdef inline int check_exceed(floating* a_dist_arr, ITYPE_t row,                        ITYPE_t* a_idx_arr, floating C,                       floating val, floating* largest_dist, floating beta                       )nogil:    cdef DTYPE_t k_float    if largest_dist[row]==INF:        if val>=a_dist_arr[row]:            k_float= a_idx_arr[row]            if val**beta * (k_float+1) <C:                a_dist_arr[row]=val                a_idx_arr[row]+=1            else:                largest_dist[row]=val        else:            a_idx_arr[row]+=1            k_float = a_idx_arr[row]            if a_dist_arr[row]**beta*(k_float+1) >=C:                largest_dist[row]=val    return 0####################################################################### Define doc strings, substituting the appropriate class name using# the DOC_DICT variable defined in the pyx files.CLASS_DOC = \"""{BinaryTree}(X, leaf_size=40, metric='minkowski', **kwargs){BinaryTree} for fast generalized N-point problemsRead more in the :ref:`User Guide <unsupervised_neighbors>`.Parameters----------X : array-like of shape (n_samples, n_features)    n_samples is the number of points in the data set, and    n_features is the dimension of the parameter space.    Note: if X is a C-contiguous array of doubles then data will    not be copied. Otherwise, an internal copy will be made.leaf_size : positive int, default=40    Number of points at which to switch to brute-force. Changing    leaf_size will not affect the results of a query, but can    significantly impact the speed of a query and the memory required    to store the constructed tree.  The amount of memory needed to    store the tree scales as approximately n_samples / leaf_size.    For a specified ``leaf_size``, a leaf node is guaranteed to    satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in    the case that ``n_samples < leaf_size``.metric : str or DistanceMetric object, default='minkowski'    Metric to use for distance computation. Default is "minkowski", which    results in the standard Euclidean distance when p = 2.    {binary_tree}.valid_metrics gives a list of the metrics which are valid for    {BinaryTree}. See the documentation of `scipy.spatial.distance    <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and the    metrics listed in :class:`~sklearn.metrics.pairwise.distance_metrics` for    more information.Additional keywords are passed to the distance metric class.Note: Callable functions in the metric parameter are NOT supported for KDTreeand Ball Tree. Function call overhead will result in very poor performance.Attributes----------data : memory view    The training dataExamples--------Query for k-nearest neighbors    >>> import numpy as np    >>> from sklearn.neighbors import {BinaryTree}    >>> rng = np.random.RandomState(0)    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions    >>> tree = {BinaryTree}(X, leaf_size=2)              # doctest: +SKIP    >>> dist, ind = tree.query(X[:1], k=3)                # doctest: +SKIP    >>> print(ind)  # indices of 3 closest neighbors    [0 3 1]    >>> print(dist)  # distances to 3 closest neighbors    [ 0.          0.19662693  0.29473397]Pickle and Unpickle a tree.  Note that the state of the tree is saved in thepickle operation: the tree needs not be rebuilt upon unpickling.    >>> import numpy as np    >>> import pickle    >>> rng = np.random.RandomState(0)    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions    >>> tree = {BinaryTree}(X, leaf_size=2)        # doctest: +SKIP    >>> s = pickle.dumps(tree)                     # doctest: +SKIP    >>> tree_copy = pickle.loads(s)                # doctest: +SKIP    >>> dist, ind = tree_copy.query(X[:1], k=3)     # doctest: +SKIP    >>> print(ind)  # indices of 3 closest neighbors    [0 3 1]    >>> print(dist)  # distances to 3 closest neighbors    [ 0.          0.19662693  0.29473397]Query for neighbors within a given radius    >>> import numpy as np    >>> rng = np.random.RandomState(0)    >>> X = rng.random_sample((10, 3))  # 10 points in 3 dimensions    >>> tree = {BinaryTree}(X, leaf_size=2)     # doctest: +SKIP    >>> print(tree.query_radius(X[:1], r=0.3, count_only=True))    3    >>> ind = tree.query_radius(X[:1], r=0.3)  # doctest: +SKIP    >>> print(ind)  # indices of neighbors within distance 0.3    [3 0 1]Compute a gaussian kernel density estimate:    >>> import numpy as np    >>> rng = np.random.RandomState(42)    >>> X = rng.random_sample((100, 3))    >>> tree = {BinaryTree}(X)                # doctest: +SKIP    >>> tree.kernel_density(X[:3], h=0.1, kernel='gaussian')    array([ 6.94114649,  7.83281226,  7.2071716 ])Compute a two-point auto-correlation function    >>> import numpy as np    >>> rng = np.random.RandomState(0)    >>> X = rng.random_sample((30, 3))    >>> r = np.linspace(0, 1, 5)    >>> tree = {BinaryTree}(X)                # doctest: +SKIP    >>> tree.two_point_correlation(X, r)    array([ 30,  62, 278, 580, 820])"""cdef class NeighborsHeap:    """A max-heap structure to keep track of distances/indices of neighbors    This implements an efficient pre-allocated set of fixed-size heaps    for chasing neighbors, holding both an index and a distance.    When any row of the heap is full, adding an additional point will push    the furthest point off the heap.    Parameters    ----------    n_pts : int        the number of heaps to use    n_nbrs : int        the size of each heap.    """    cdef cnp.ndarray distances_arr    cdef cnp.ndarray indices_arr    cdef DTYPE_t[:, ::1] distances    cdef ITYPE_t[:, ::1] indices    def __cinit__(self):        self.distances_arr = np.zeros((1, 1), dtype=DTYPE, order='C')        self.indices_arr = np.zeros((1, 1), dtype=ITYPE, order='C')                self.distances = self.distances_arr        self.indices = self.indices_arr                 def __init__(self, n_pts, n_nbrs):        self.distances_arr = np.full((n_pts, n_nbrs), np.inf, dtype=DTYPE,                                     order='C')        self.indices_arr = np.zeros((n_pts, n_nbrs), dtype=ITYPE, order='C')        self.distances = self.distances_arr        self.indices = self.indices_arr                                 def get_arrays(self, sort=True):        """Get the arrays of distances and indices within the heap.        If sort=True, then simultaneously sort the indices and distances,        so the closer points are listed first.        """        if sort:            self._sort()        return self.distances_arr, self.indices_arr        cdef inline DTYPE_t largest(self, ITYPE_t row) nogil except -1:        """Return the largest distance in the given row"""        return self.distances[row, 0]    def push(self, ITYPE_t row, DTYPE_t val, ITYPE_t i_val):                return self._push(row, val, i_val)            cdef int _push(self, ITYPE_t row, DTYPE_t val,                   ITYPE_t i_val) nogil except -1:        """push (val, i_val) into the given row"""        cdef:            ITYPE_t size = self.distances.shape[1]            DTYPE_t* dist_arr = &self.distances[row, 0]            ITYPE_t* ind_arr = &self.indices[row, 0]        return heap_push(dist_arr, ind_arr, size, val, i_val)    cdef int _sort(self) except -1:        """simultaneously sort the distances and indices"""        cdef DTYPE_t[:, ::1] distances = self.distances        cdef ITYPE_t[:, ::1] indices = self.indices        cdef ITYPE_t row        for row in range(distances.shape[0]):            simultaneous_sort(&distances[row, 0],                               &indices[row, 0],                               distances.shape[1])        return 0cdef class AdaptiveNeighborsHeap:    """A max-heap structure to keep track of distances/indices of neighbors    which allows adaptive neighbor searching        Parameters    ----------    n_pts : int        the number of heaps to use    n_nbrs : int        the size of each heap.    C : float        threshold constant    beta : float        exponential constant     """    cdef cnp.ndarray distances_arr    cdef cnp.ndarray indices_arr    cdef cnp.ndarray adaptive_max_idx_arr    cdef cnp.ndarray adaptive_max_distance_arr    cdef cnp.ndarray largest_dist_arr            cdef ITYPE_t[::1] adaptive_max_idx    cdef DTYPE_t[::1] adaptive_max_distance    cdef DTYPE_t[::1] largest_dist    cdef DTYPE_t[:, ::1] distances    cdef ITYPE_t[:, ::1] indices    cdef DTYPE_t C    cdef DTYPE_t beta    def __cinit__(self):        self.distances_arr = np.zeros((1, 1), dtype=DTYPE, order='C')        self.indices_arr = np.zeros((1, 1), dtype=ITYPE, order='C')                self.distances = self.distances_arr        self.indices = self.indices_arr                self.adaptive_max_idx_arr = np.zeros(2, dtype=ITYPE, order='C')        self.adaptive_max_distance_arr = np.zeros(2, dtype=DTYPE, order='C')                self.adaptive_max_idx = self.adaptive_max_idx_arr        self.adaptive_max_distance = self.adaptive_max_distance_arr                self.C = 1.5        self.beta = 1.5                self.largest_dist_arr = np.zeros(1, dtype=DTYPE, order='C')        self.largest_dist=self.largest_dist_arr                    # TODO: assert beta and C are positive    def __init__(self, n_pts, n_nbrs, C, beta):        self.distances_arr = np.full((n_pts, n_nbrs), np.inf, dtype=DTYPE,                                     order='C')        self.indices_arr = np.zeros((n_pts, n_nbrs), dtype=ITYPE, order='C')        self.distances = self.distances_arr        self.indices = self.indices_arr                self.adaptive_max_idx_arr = np.full(n_pts, 0,dtype=ITYPE)        self.adaptive_max_idx = self.adaptive_max_idx_arr                self.adaptive_max_distance_arr= np.full(n_pts, 0,dtype=DTYPE)        self.adaptive_max_distance=self.adaptive_max_distance_arr                         self.C = C        self.beta = beta                self.largest_dist_arr = np.full(n_pts,np.inf, dtype=DTYPE, order='C')        self.largest_dist = self.largest_dist_arr                            def get_arrays(self, sort=True):        """Get the arrays of distances and indices within the heap.        If sort=True, then simultaneously sort the indices and distances,        so the closer points are listed first.        """        if sort:            self._sort()        return self.distances_arr, self.indices_arr        cdef inline DTYPE_t largest(self, ITYPE_t row) nogil except -1:        """Return the largest distance in the given row"""        return self.largest_dist[row]            def push(self, ITYPE_t row, DTYPE_t val, ITYPE_t i_val):        return self._push(row, val, i_val)    cdef int _push(self, ITYPE_t row, DTYPE_t val,                   ITYPE_t i_val) nogil except -1:        """push (val, i_val) into the given row"""                       cdef DTYPE_t* a_dist_arr =&self.adaptive_max_distance[0]        cdef ITYPE_t* a_idx_arr = &self.adaptive_max_idx[0]        cdef DTYPE_t threshold_const = self.C        cdef DTYPE_t* largest_dist= &self.largest_dist[0]                cdef DTYPE_t beta= self.beta                                                   check_exceed(a_dist_arr, row, a_idx_arr, threshold_const, val, largest_dist, beta)                                        cdef:            ITYPE_t size = self.distances.shape[1]            DTYPE_t* dist_arr = &self.distances[row, 0]            ITYPE_t* ind_arr = &self.indices[row, 0]        return heap_push(dist_arr, ind_arr, size, val, i_val)    cdef int _sort(self) except -1:        """simultaneously sort the distances and indices"""        cdef DTYPE_t[:, ::1] distances = self.distances        cdef ITYPE_t[:, ::1] indices = self.indices        cdef ITYPE_t row        for row in range(distances.shape[0]):            simultaneous_sort(&distances[row, 0],                               &indices[row, 0],                               distances.shape[1])        return 0#------------------------------------------------------------# find_node_split_dim:#  this computes the equivalent of#  j_max = np.argmax(np.max(data, 0) - np.min(data, 0))cdef ITYPE_t find_node_split_dim(DTYPE_t* data,                                 ITYPE_t* node_indices,                                 ITYPE_t n_features,                                 ITYPE_t n_points) except -1:    """Find the dimension with the largest spread.    Parameters    ----------    data : double pointer        Pointer to a 2D array of the training data, of shape [N, n_features].        N must be greater than any of the values in node_indices.    node_indices : int pointer        Pointer to a 1D array of length n_points.  This lists the indices of        each of the points within the current node.    Returns    -------    i_max : int        The index of the feature (dimension) within the node that has the        largest spread.    Notes    -----    In numpy, this operation is equivalent to    def find_node_split_dim(data, node_indices):        return np.argmax(data[node_indices].max(0) - data[node_indices].min(0))    The cython version is much more efficient in both computation and memory.    """    cdef DTYPE_t min_val, max_val, val, spread, max_spread    cdef ITYPE_t i, j, j_max    j_max = 0    max_spread = 0    for j in range(n_features):        max_val = data[node_indices[0] * n_features + j]        min_val = max_val        for i in range(1, n_points):            val = data[node_indices[i] * n_features + j]            max_val = fmax(max_val, val)            min_val = fmin(min_val, val)        spread = max_val - min_val        if spread > max_spread:            max_spread = spread            j_max = j    return j_max####################################################################### NodeHeap : min-heap used to keep track of nodes during#            breadth-first querycdef inline void swap_nodes(NodeHeapData_t* arr, ITYPE_t i1, ITYPE_t i2):    cdef NodeHeapData_t tmp = arr[i1]    arr[i1] = arr[i2]    arr[i2] = tmpcdef class NodeHeap:    """NodeHeap    This is a min-heap implementation for keeping track of nodes    during a breadth-first search.  Unlike the NeighborsHeap above,    the NodeHeap does not have a fixed size and must be able to grow    as elements are added.    Internally, the data is stored in a simple binary heap which meets    the min heap condition:        heap[i].val < min(heap[2 * i + 1].val, heap[2 * i + 2].val)    """    cdef cnp.ndarray data_arr    cdef NodeHeapData_t[::1] data    cdef ITYPE_t n    def __cinit__(self):        self.data_arr = np.zeros(1, dtype=NodeHeapData, order='C')        self.data = self.data_arr    def __init__(self, size_guess=100):        size_guess = max(size_guess, 1)  # need space for at least one item        self.data_arr = np.zeros(size_guess, dtype=NodeHeapData, order='C')        self.data = self.data_arr        self.n = size_guess        self.clear()    cdef int resize(self, ITYPE_t new_size) except -1:        """Resize the heap to be either larger or smaller"""        cdef NodeHeapData_t *data_ptr        cdef NodeHeapData_t *new_data_ptr        cdef ITYPE_t i        cdef ITYPE_t size = self.data.shape[0]        cdef cnp.ndarray new_data_arr = np.zeros(new_size,                                                dtype=NodeHeapData)        cdef NodeHeapData_t[::1] new_data = new_data_arr        if size > 0 and new_size > 0:            data_ptr = &self.data[0]            new_data_ptr = &new_data[0]            for i in range(min(size, new_size)):                new_data_ptr[i] = data_ptr[i]        if new_size < size:            self.n = new_size        self.data = new_data        self.data_arr = new_data_arr        return 0    cdef int push(self, NodeHeapData_t data) except -1:        """Push a new item onto the heap"""        cdef ITYPE_t i, i_parent        cdef NodeHeapData_t* data_arr        self.n += 1        if self.n > self.data.shape[0]:            self.resize(2 * self.n)        # put the new element at the end,        # and then perform swaps until the heap is in order        data_arr = &self.data[0]        i = self.n - 1        data_arr[i] = data        while i > 0:            i_parent = (i - 1) // 2            if data_arr[i_parent].val <= data_arr[i].val:                break            else:                swap_nodes(data_arr, i, i_parent)                i = i_parent        return 0    cdef NodeHeapData_t peek(self):        """Peek at the root of the heap, without removing it"""        return self.data[0]    cdef NodeHeapData_t pop(self):        """Remove the root of the heap, and update the remaining nodes"""        if self.n == 0:            raise ValueError('cannot pop on empty heap')        cdef ITYPE_t i, i_child1, i_child2, i_swap        cdef NodeHeapData_t* data_arr = &self.data[0]        cdef NodeHeapData_t popped_element = data_arr[0]        # pop off the first element, move the last element to the front,        # and then perform swaps until the heap is back in order        data_arr[0] = data_arr[self.n - 1]        self.n -= 1        i = 0        while (i < self.n):            i_child1 = 2 * i + 1            i_child2 = 2 * i + 2            i_swap = 0            if i_child2 < self.n:                if data_arr[i_child1].val <= data_arr[i_child2].val:                    i_swap = i_child1                else:                    i_swap = i_child2            elif i_child1 < self.n:                i_swap = i_child1            else:                break            if (i_swap > 0) and (data_arr[i_swap].val <= data_arr[i].val):                swap_nodes(data_arr, i, i_swap)                i = i_swap            else:                break        return popped_element    cdef void clear(self):        """Clear the heap"""        self.n = 0####################################################################### newObj function#  this is a helper function for picklingdef newObj(obj):    return obj.__new__(obj)####################################################################### Binary Tree classcdef class BinaryTree:    cdef cnp.ndarray data_arr       cdef cnp.ndarray idx_array_arr    cdef cnp.ndarray node_data_arr    cdef cnp.ndarray node_bounds_arr    cdef readonly const DTYPE_t[:, ::1] data    cdef public DTYPE_t sum_weight    # Even if those memoryviews attributes are const-qualified,    # they get modified via their numpy counterpart.    # For instance, `node_data` gets modified via `node_data_arr`.    cdef public const ITYPE_t[::1] idx_array    cdef public const NodeData_t[::1] node_data    cdef public const DTYPE_t[:, :, ::1] node_bounds    cdef ITYPE_t leaf_size    cdef ITYPE_t n_levels    cdef ITYPE_t n_nodes        cdef int euclidean    # variables to keep track of building & querying stats    cdef int n_trims    cdef int n_leaves    cdef int n_splits    cdef int n_calls    # Use cinit to initialize all arrays to empty: this will prevent memory    # errors and seg-faults in rare cases where __init__ is not called    def __cinit__(self):        self.data_arr = np.empty((1, 1), dtype=DTYPE, order='C')               self.idx_array_arr = np.empty(1, dtype=ITYPE, order='C')        self.node_data_arr = np.empty(1, dtype=NodeData, order='C')        self.node_bounds_arr = np.empty((1, 1, 1), dtype=DTYPE)        self.data = self.data_arr                self.idx_array = self.idx_array_arr        self.node_data = self.node_data_arr        self.node_bounds = self.node_bounds_arr        self.leaf_size = 0        self.n_levels = 0        self.n_nodes = 0        self.euclidean = False        self.n_trims = 0        self.n_leaves = 0        self.n_splits = 0        self.n_calls = 0    def __init__(self, data,                 leaf_size=40, metric='euclidean', **kwargs):        # validate data        self.data_arr = check_array(data, dtype=DTYPE, order='C')        if self.data_arr.size == 0:            raise ValueError("X is an empty array")        n_samples = self.data_arr.shape[0]        n_features = self.data_arr.shape[1]        if leaf_size < 1:            raise ValueError("leaf_size must be greater than or equal to 1")        self.leaf_size = leaf_size                   # determine number of levels in the tree, and from this        # the number of nodes in the tree.  This results in leaf nodes        # with numbers of points between leaf_size and 2 * leaf_size        self.n_levels = int(            np.log2(fmax(1, (n_samples - 1) / self.leaf_size)) + 1)        self.n_nodes = (2 ** self.n_levels) - 1        # allocate arrays for storage        self.idx_array_arr = np.arange(n_samples, dtype=ITYPE)        self.node_data_arr = np.zeros(self.n_nodes, dtype=NodeData)            self._update_memviews()        # Allocate tree-specific data        allocate_data(self, self.n_nodes, n_features)        self._recursive_build(            node_data=self.node_data_arr,            i_node=0,            idx_start=0,            idx_end=n_samples        )    def _update_memviews(self):        self.data = self.data_arr        self.idx_array = self.idx_array_arr        self.node_data = self.node_data_arr        self.node_bounds = self.node_bounds_arr    def get_arrays(self):        """        get_arrays()        Get data and node arrays.        Returns        -------        arrays: tuple of array            Arrays for storing tree data, index, node data and node bounds.        """        return (self.data_arr, self.idx_array_arr,                self.node_data_arr, self.node_bounds_arr)    cdef inline DTYPE_t dist(self, DTYPE_t* x1, DTYPE_t* x2,                             ITYPE_t size) nogil except -1:        """Compute the distance between arrays x1 and x2"""        self.n_calls += 1        return euclidean_dist(x1, x2, size)           cdef inline DTYPE_t rdist(self, DTYPE_t* x1, DTYPE_t* x2,                              ITYPE_t size) nogil except -1:        """Compute the reduced distance between arrays x1 and x2.        The reduced distance, defined for some metrics, is a quantity which        is more efficient to compute than the distance, but preserves the        relative rankings of the true distance.  For example, the reduced        distance for the Euclidean metric is the squared-euclidean distance.        """        self.n_calls += 1        return euclidean_rdist(x1, x2, size)          cdef int _recursive_build(self, NodeData_t[::1] node_data, ITYPE_t i_node, ITYPE_t idx_start,                              ITYPE_t idx_end) except -1:        """Recursively build the tree.        Parameters        ----------        i_node : int            the node for the current step        idx_start, idx_end : int            the bounding indices in the idx_array which define the points that            belong to this node.        """        cdef ITYPE_t imax        cdef ITYPE_t n_features = self.data.shape[1]        cdef ITYPE_t n_points = idx_end - idx_start        cdef ITYPE_t n_mid = n_points / 2        cdef ITYPE_t* idx_array = &self.idx_array[idx_start]        cdef DTYPE_t* data = &self.data[0, 0]        # initialize node data        init_node(self, node_data, i_node, idx_start, idx_end)        if 2 * i_node + 1 >= self.n_nodes:            node_data[i_node].is_leaf = True            if idx_end - idx_start > 2 * self.leaf_size:                # this shouldn't happen if our memory allocation is correct                # we'll proactively prevent memory errors, but raise a                # warning saying we're doing so.                import warnings                warnings.warn("Internal: memory layout is flawed: "                              "not enough nodes allocated")        elif idx_end - idx_start < 2:            # again, this shouldn't happen if our memory allocation            # is correct.  Raise a warning.            import warnings            warnings.warn("Internal: memory layout is flawed: "                          "too many nodes allocated")            node_data[i_node].is_leaf = True        else:            # split node and recursively construct child nodes.            node_data[i_node].is_leaf = False            i_max = find_node_split_dim(data, idx_array,                                        n_features, n_points)            partition_node_indices(data, idx_array, i_max, n_mid,                                   n_features, n_points)            self._recursive_build(node_data,2 * i_node + 1,                                  idx_start, idx_start + n_mid)            self._recursive_build(node_data, 2 * i_node + 2,                                  idx_start + n_mid, idx_end)    def query(self, X, k=1, return_distance=True,              dualtree=False, breadth_first=False,              sort_results=True):        """        query(X, k=1, return_distance=True, breadth_first=False)        query the tree for the k nearest neighbors        Parameters        ----------        X : array-like of shape (n_samples, n_features)            An array of points to query        k : int, default=1            The number of nearest neighbors to return        return_distance : bool, default=True            if True, return a tuple (d, i) of distances and indices            if False, return array i        breadth_first : bool, default=False            if True, then query the nodes in a breadth-first manner.            Otherwise, query the nodes in a depth-first manner.        sort_results : bool, default=True            if True, then distances and indices of each point are sorted            on return, so that the first column contains the closest points.            Otherwise, neighbors are returned in an arbitrary order.        Returns        -------        i    : if return_distance == False        (d,i) : if return_distance == True        d : ndarray of shape X.shape[:-1] + (k,), dtype=double            Each entry gives the list of distances to the neighbors of the            corresponding point.        i : ndarray of shape X.shape[:-1] + (k,), dtype=int            Each entry gives the list of indices of neighbors of the            corresponding point.        """        # XXX: we should allow X to be a pre-built tree.        X = check_array(X, dtype=DTYPE, order='C')        if X.shape[X.ndim - 1] != self.data.shape[1]:            raise ValueError("query data dimension must "                             "match training data dimension")        if self.data.shape[0] < k:            raise ValueError("k must be less than or equal "                             "to the number of training points")        # flatten X, and save original shape information        np_Xarr = X.reshape((-1, self.data.shape[1]))        cdef const DTYPE_t[:, ::1] Xarr = np_Xarr        cdef DTYPE_t reduced_dist_LB        cdef ITYPE_t i        cdef DTYPE_t* pt        # initialize heap for neighbors        cdef NeighborsHeap heap = NeighborsHeap(Xarr.shape[0], k)        # node heap for breadth-first queries        cdef NodeHeap nodeheap        if breadth_first:            nodeheap = NodeHeap(self.data.shape[0] // self.leaf_size)        # bounds is needed for the dual tree algorithm        cdef DTYPE_t[::1] bounds        self.n_trims = 0        self.n_leaves = 0        self.n_splits = 0               pt = &Xarr[0, 0]        if breadth_first:            for i in range(Xarr.shape[0]):                self._query_single_breadthfirst(pt, i, heap, nodeheap)                pt += Xarr.shape[1]        else:            with nogil:                for i in range(Xarr.shape[0]):                    reduced_dist_LB = min_rdist(self, 0, pt)                    self._query_single_depthfirst(0, pt, i, heap,                                                  reduced_dist_LB)                    pt += Xarr.shape[1]        distances, indices = heap.get_arrays(sort=sort_results)        #distances = self.dist_metric.rdist_to_dist(distances)        distances = distances**0.5                # deflatten results        if return_distance:            return (distances.reshape(X.shape[:X.ndim - 1] + (k,)),                    indices.reshape(X.shape[:X.ndim - 1] + (k,)))        else:            return indices.reshape(X.shape[:X.ndim - 1] + (k,))       def adaptive_query(self, X, C=1, beta=1,  max_neighbor=100,              sort_results=True):        """        adaptive_query(X, C=1, beta=1, max_neighbor=100, sort_result=True)              max_neighbor        adaptive query the tree for the k nearest neighbors        Parameters        ----------        X : array-like of shape (n_samples, n_features)            An array of points to query        C : float, default=1            Thresholding constant         beta : float, default =1            Exponentail constant        max_neighbor : int, default =100            Maximum number of neighbor to search        sort_results : bool, default=True            if True, then distances and indices of each point are sorted            on return, so that the first column contains the closest points.            Otherwise, neighbors are returned in an arbitrary order.        Returns        -------        (dist,k)                dist : ndarray of shape (X.shape[0],), dtype=double            Each entry gives the list of distances which is the searching             result of the corresponding point.        k : ndarray of shape (X.shape[0],), dtype=int            Each entry gives the list of number of neighbors which is the searching             result of the corresponding point.        """        # XXX: we should allow X to be a pre-built tree.        X = check_array(X, dtype=DTYPE, order='C')                return_shape=min(self.data.shape[0],max_neighbor)        if X.shape[X.ndim - 1] != self.data.shape[1]:            raise ValueError("query data dimension must "                             "match training data dimension")          # flatten X, and save original shape information        np_Xarr = X.reshape((-1, self.data.shape[1]))        cdef const DTYPE_t[:, ::1] Xarr = np_Xarr        cdef DTYPE_t reduced_dist_LB        cdef ITYPE_t i        cdef DTYPE_t* pt                        cdef DTYPE_t C_ctype = C        cdef DTYPE_t beta_ctype = beta              # initialize heap for neighbors        cdef AdaptiveNeighborsHeap heap = AdaptiveNeighborsHeap(Xarr.shape[0], return_shape, C_ctype, beta_ctype)                self.n_trims = 0        self.n_leaves = 0        self.n_splits = 0               pt = &Xarr[0, 0]        with nogil:            for i in range(Xarr.shape[0]):                reduced_dist_LB = min_rdist(self, 0, pt)                self._adaptive_query_single_depthfirst(0, pt, i, heap,                                              reduced_dist_LB)                pt += Xarr.shape[1]                                     distances, indices = heap.get_arrays(sort=sort_results)        distances = distances**0.5            adaptive_distance=[]        adaptive_number=[]        for i in range(distances.shape[0]):            statistic_vec=[distances[i,j]**beta*(j+1) for j in range(len(distances[i]))]            non_inf_idx=np.array(statistic_vec)<C            if non_inf_idx.sum()==0:                raise ValueError("Given condition is too restrctive, no number of neighbors feasible")            else:                adaptive_number.append(non_inf_idx.sum())                adaptive_distance.append(distances[i][non_inf_idx].max())                return np.array(adaptive_distance),np.array(adaptive_number)    cdef int _query_single_depthfirst(self, ITYPE_t i_node,                                      DTYPE_t* pt, ITYPE_t i_pt,                                      NeighborsHeap heap,                                      DTYPE_t reduced_dist_LB) nogil except -1:        """Recursive Single-tree k-neighbors query, depth-first approach"""        cdef NodeData_t node_info = self.node_data[i_node]        cdef DTYPE_t dist_pt, reduced_dist_LB_1, reduced_dist_LB_2        cdef ITYPE_t i, i1, i2        cdef DTYPE_t* data = &self.data[0, 0]        #------------------------------------------------------------        # Case 1: query point is outside node radius:        #         trim it from the query        if reduced_dist_LB > heap.largest(i_pt):            self.n_trims += 1        #------------------------------------------------------------        # Case 2: this is a leaf node.  Update set of nearby points        elif node_info.is_leaf:            self.n_leaves += 1            for i in range(node_info.idx_start, node_info.idx_end):                dist_pt = self.rdist(pt,                                     &self.data[self.idx_array[i], 0],                                     self.data.shape[1])                heap._push(i_pt, dist_pt, self.idx_array[i])        #------------------------------------------------------------        # Case 3: Node is not a leaf.  Recursively query subnodes        #         starting with the closest        else:            self.n_splits += 1            i1 = 2 * i_node + 1            i2 = i1 + 1            reduced_dist_LB_1 = min_rdist(self, i1, pt)            reduced_dist_LB_2 = min_rdist(self, i2, pt)            # recursively query subnodes            if reduced_dist_LB_1 <= reduced_dist_LB_2:                self._query_single_depthfirst(i1, pt, i_pt, heap,                                              reduced_dist_LB_1)                self._query_single_depthfirst(i2, pt, i_pt, heap,                                              reduced_dist_LB_2)            else:                self._query_single_depthfirst(i2, pt, i_pt, heap,                                              reduced_dist_LB_2)                self._query_single_depthfirst(i1, pt, i_pt, heap,                                              reduced_dist_LB_1)        return 0    cdef int _query_single_breadthfirst(self, DTYPE_t* pt,                                        ITYPE_t i_pt,                                        NeighborsHeap heap,                                        NodeHeap nodeheap) except -1:        """Non-recursive single-tree k-neighbors query, breadth-first search"""        cdef ITYPE_t i, i_node        cdef DTYPE_t dist_pt, reduced_dist_LB        cdef NodeData_t* node_data = &self.node_data[0]        cdef DTYPE_t* data = &self.data[0, 0]        # Set up the node heap and push the head node onto it        cdef NodeHeapData_t nodeheap_item        nodeheap_item.val = min_rdist(self, 0, pt)        nodeheap_item.i1 = 0        nodeheap.push(nodeheap_item)        while nodeheap.n > 0:            nodeheap_item = nodeheap.pop()            reduced_dist_LB = nodeheap_item.val            i_node = nodeheap_item.i1            node_info = node_data[i_node]            #------------------------------------------------------------            # Case 1: query point is outside node radius:            #         trim it from the query            if reduced_dist_LB > heap.largest(i_pt):                self.n_trims += 1            #------------------------------------------------------------            # Case 2: this is a leaf node.  Update set of nearby points            elif node_data[i_node].is_leaf:                self.n_leaves += 1                for i in range(node_data[i_node].idx_start,                               node_data[i_node].idx_end):                    dist_pt = self.rdist(pt,                                         &self.data[self.idx_array[i], 0],                                         self.data.shape[1])                    heap._push(i_pt, dist_pt, self.idx_array[i])            #------------------------------------------------------------            # Case 3: Node is not a leaf.  Add subnodes to the node heap            else:                self.n_splits += 1                for i in range(2 * i_node + 1, 2 * i_node + 3):                    nodeheap_item.i1 = i                    nodeheap_item.val = min_rdist(self, i, pt)                    nodeheap.push(nodeheap_item)        return 0    cdef int _adaptive_query_single_depthfirst(self, ITYPE_t i_node,                                      DTYPE_t* pt, ITYPE_t i_pt,                                      AdaptiveNeighborsHeap heap,                                      DTYPE_t reduced_dist_LB) nogil except -1:                        """Recursive Single-tree k-neighbors adaptive query, depth-first approach"""        cdef NodeData_t node_info = self.node_data[i_node]                cdef DTYPE_t dist_pt, reduced_dist_LB_1, reduced_dist_LB_2        cdef ITYPE_t i, i1, i2        cdef DTYPE_t* data = &self.data[0, 0]        #------------------------------------------------------------        # Case 1: query point is outside node radius:        #         trim it from the query        if reduced_dist_LB > heap.largest(i_pt):            self.n_trims += 1        #------------------------------------------------------------        # Case 2: this is a leaf node.  Update set of nearby points        elif node_info.is_leaf:            self.n_leaves += 1            for i in range(node_info.idx_start, node_info.idx_end):                dist_pt = self.rdist(pt,                                     &self.data[self.idx_array[i], 0],                                     self.data.shape[1])                                            heap._push(i_pt, dist_pt, self.idx_array[i])                       #------------------------------------------------------------        # Case 3: Node is not a leaf.  Recursively query subnodes        #         starting with the closest        else:            self.n_splits += 1            i1 = 2 * i_node + 1            i2 = i1 + 1            reduced_dist_LB_1 = min_rdist(self, i1, pt)            reduced_dist_LB_2 = min_rdist(self, i2, pt)            # recursively query subnodes            if reduced_dist_LB_1 <= reduced_dist_LB_2:                self._adaptive_query_single_depthfirst(i1, pt, i_pt, heap,                                              reduced_dist_LB_1)                self._adaptive_query_single_depthfirst(i2, pt, i_pt, heap,                                              reduced_dist_LB_2)            else:                self._adaptive_query_single_depthfirst(i2, pt, i_pt, heap,                                              reduced_dist_LB_2)                self._adaptive_query_single_depthfirst(i1, pt, i_pt, heap,                                              reduced_dist_LB_1)        return 0      